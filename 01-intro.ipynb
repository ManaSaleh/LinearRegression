{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00a91eb-1694-4f29-9e20-f8d84508925e",
   "metadata": {},
   "source": [
    "# **Introduction to Linear Regression**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed60cca0",
   "metadata": {},
   "source": [
    "<img src='images/LinearRelationship.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db32ff-9cbe-4a5f-b932-56dfe58f8141",
   "metadata": {},
   "source": [
    "### **What is Linear Regression?**\n",
    "Linear Regression is one of the simplest and most widely used algorithms in machine learning. It models the relationship between one or more input features and a target variable by fitting a straight line or hyperplane through the data.\n",
    "\n",
    "In its simplest form, Linear Regression models the relationship between a single input feature $(x)$ and the target variable $(y$):\n",
    "\n",
    "$\n",
    "y = \\theta_0 + \\theta_1 \\cdot x\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $(y)$: Target variable (dependent variable).\n",
    "- $(x)$: Input feature (independent variable).\n",
    "- $(\\theta_0)$: Bias term or intercept.\n",
    "- $(\\theta_1)$: Weight or coefficient for $(x)$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f81223-c0fa-499a-b49a-fd53c7a9540f",
   "metadata": {},
   "source": [
    "### **Generalized Form**\n",
    "For multiple input features $((x_1, x_2, \\ldots, x_n))$, Linear Regression predicts $(y)$ as a weighted sum of the features plus a bias term:\n",
    "\n",
    "$\n",
    "y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_n x_n\n",
    "$\n",
    "\n",
    "This can also be written in a **vectorized form**:\n",
    "\n",
    "$\n",
    "y = h_\\theta(x) = \\theta \\cdot x\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $(\\theta = [\\theta_0, \\theta_1, \\ldots, \\theta_n])$: Parameter vector, including the bias term.\n",
    "- $(x = [x_0, x_1, \\ldots, x_n])$: Feature vector, where $(x_0 = 1)$ for the bias term.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08182e3b-d261-415c-98d7-25b3d2e50e6b",
   "metadata": {},
   "source": [
    "### **Understanding the Components**\n",
    "- **Bias Term $((\\theta_0))$**:\n",
    "  - Represents the intercept of the line or hyperplane.\n",
    "  - Adjusts the line's position relative to the origin.\n",
    "  \n",
    "- **Weights $((\\theta_1, \\theta_2, \\ldots, \\theta_n))$**:\n",
    "  - Determine the slope of the line or hyperplane.\n",
    "  - Represent the impact of each feature on the target variable.\n",
    "\n",
    "- **Feature Vector $((x))$**:\n",
    "  - Contains all the input features for a single data instance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abbee5-ceb8-4d67-835e-ca615099d806",
   "metadata": {},
   "source": [
    "### **Making Predictions**\n",
    "To predict the target variable $((\\hat{y}\))$ for a given instance:\n",
    "1. Compute the dot product of $\(\\theta\)$ and $\(x\)$.\n",
    "2. Add the bias term $\(\\theta_0\)$.\n",
    "\n",
    "For example:\n",
    "\n",
    "$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_n x_n\n",
    "$\n",
    "\n",
    "Or, using vectorized notation:\n",
    "\n",
    "$\n",
    "\\hat{y} = \\theta \\cdot x\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $(\\hat{y})$: Predicted value.\n",
    "- $(\\theta)$: Parameter vector.\n",
    "- $(x)$: Feature vector.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc719e6-dd09-4e98-99c0-6a5da9db83e0",
   "metadata": {},
   "source": [
    "### **Applications of Linear Regression**\n",
    "Linear Regression is widely used for:\n",
    "1. **Predictive Modeling**:\n",
    "   - Predicting house prices based on size, location, and other features.\n",
    "2. **Trend Analysis**:\n",
    "   - Analyzing sales trends over time.\n",
    "3. **Risk Assessment**:\n",
    "   - Estimating insurance premiums based on customer data.\n",
    "4. **Real-World Scenarios**:\n",
    "   - Estimating life satisfaction based on GDP per capita.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3633577-04ff-45a1-bdc8-2a069895d4ea",
   "metadata": {},
   "source": [
    "### **Example: Life Satisfaction Prediction**\n",
    "Suppose we want to predict life satisfaction (\\(y\\)) based on GDP per capita (\\(x\\)):\n",
    "\n",
    "$\n",
    "y = \\theta_0 + \\theta_1 \\cdot x\n",
    "$\n",
    "\n",
    "- If $(\\theta_0 = 2)$ and $(\\theta_1 = 0.5)$, then the model predicts:\n",
    "\n",
    "$\n",
    "y = 2 + 0.5 \\cdot x\n",
    "$\n",
    "\n",
    "For $(x = 10,000)$ (GDP per capita in USD):\n",
    "\n",
    "$\n",
    "y = 2 + 0.5 \\cdot 10,000 = 5,002\n",
    "$\n",
    "\n",
    "Thus, the predicted life satisfaction score is 5,002.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea4bec-3f3f-4a82-9c8f-0c37e15d3f4f",
   "metadata": {},
   "source": [
    "### **Advantages of Linear Regression**\n",
    "1. **Simplicity**:\n",
    "   - Easy to understand and implement.\n",
    "2. **Interpretability**:\n",
    "   - Clear understanding of the relationship between features and the target.\n",
    "3. **Efficiency**:\n",
    "   - Computationally efficient for small to medium-sized datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba5a8d-e7ef-47c6-b453-e1108b725911",
   "metadata": {},
   "source": [
    "### **Limitations of Linear Regression**\n",
    "1. **Assumption of Linearity**:\n",
    "   - Assumes a linear relationship between features and the target.\n",
    "2. **Sensitivity to Outliers**:\n",
    "   - Outliers can significantly affect the model’s predictions.\n",
    "3. **Multicollinearity**:\n",
    "   - Highly correlated features can destabilize coefficient estimates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd830349-8ad9-4f7d-9728-f03807860195",
   "metadata": {},
   "source": [
    "### **Key Takeaways**\n",
    "1. Linear Regression predicts a continuous target variable by fitting a line or hyperplane.\n",
    "2. It is represented mathematically as $(y = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_n x_n)$ or in vectorized form as $(y = \\theta \\cdot x)$.\n",
    "3. Widely used for predictive modeling, trend analysis, and real-world forecasting.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff149e25-2102-499f-9e01-4d3df2cf6986",
   "metadata": {},
   "source": [
    "# Resources for Understanding Bias-Variance Tradeoff\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the balance between two types of errors that affect a model's performance:\n",
    "\n",
    "- **Bias**: Error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause underfitting, where the model fails to capture the underlying patterns in the data.\n",
    "\n",
    "- **Variance**: Error introduced by the model's sensitivity to small fluctuations in the training data. High variance can lead to overfitting, where the model captures noise in the data rather than the intended outputs.\n",
    "\n",
    "Managing this tradeoff is crucial for developing models that generalize well to unseen data.\n",
    "\n",
    "## Recommended Resources\n",
    "1. **[Bias–Variance Tradeoff (Wikipedia)](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)**  \n",
    "   This Wikipedia article provides an in-depth explanation of the bias-variance tradeoff, including mathematical formulations and visual illustrations.\n",
    "\n",
    "2. **[Bias-Variance Tradeoff - Stanford University](https://cs229.stanford.edu/notes2022fall/bias-variance-annotated.pdf)**  \n",
    "   These lecture notes from Stanford's CS229 course offer a detailed discussion on the topic, complete with annotated slides and examples.\n",
    "\n",
    "3. **[Bias-Variance Tradeoff - Machine Learning Plus](https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/)**  \n",
    "   This article offers a clear explanation of the bias-variance tradeoff, including practical examples and visual aids.\n",
    "\n",
    "These resources provide valuable insights into the bias-variance tradeoff, helping you understand how to balance model complexity and generalization in your machine learning projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656a3b2-3073-4558-8212-bef3dfdc065b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Suggested Practice Platforms:\n",
    "1. **[Google Colab](https://colab.research.google.com/)**  \n",
    "   Practice Python examples directly in the cloud for free. No installation is required, and you can use pre-installed libraries and GPUs for experiments.\n",
    "\n",
    "2. **[Kaggle Notebooks](https://www.kaggle.com/code)**  \n",
    "   Access pre-written notebooks on feature scaling and polynomial features, or create your own notebooks with datasets from Kaggle.\n",
    "\n",
    "3. **Your Local Device**  \n",
    "   - **Install Python**: Download from [python.org](https://www.python.org/downloads/).\n",
    "   - **Install VS Code**: Download from [Visual Studio Code](https://code.visualstudio.com/).\n",
    "   - **Install Conda**: Use [Anaconda](https://www.anaconda.com/products/distribution) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html).\n",
    "   - **Install Jupyter Notebook**: Install it via Conda or Pip and practice locally with full control over your environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
